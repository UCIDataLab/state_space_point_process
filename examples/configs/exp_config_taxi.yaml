pipeline_config_id: runner_config

data:
  taxi:
    data_format: json
    train_dir:  easytpp/taxi
    valid_dir:  easytpp/taxi
    test_dir:  easytpp/taxi
    data_specs:
      num_event_types: 10
      pad_token_id: 10
      padding_side: right


RMTPP_train:
  base_config:
    stage: train
    backend: torch
    dataset_id: taxi
    runner_id: std_tpp
    model_id: RMTPP
    base_dir: './checkpoints/'
  trainer_config:
    batch_size: 256
    max_epoch: 300
    shuffle: True
    optimizer: adam
    learning_rate: 1.e-2
    valid_freq: 1
    use_tfb: False
    metrics: [ 'acc', 'rmse' ]
    seed: 2019
    gpu: 0
    lr_scheduler: True
    weight_decay: 0.
    warmup_pct: 0.01
    lr_decay_style: 'cosine'
  model_config:
    hidden_size: 128
    sharing_param_layer: False
    loss_integral_num_sample_per_step: 10
    use_mc_samples: True
    dropout: 0.0


NHP_train:
  base_config:
    stage: train
    backend: torch
    dataset_id: taxi
    runner_id: std_tpp
    model_id: NHP
    base_dir: './checkpoints/'
  trainer_config:
    batch_size: 256
    max_epoch: 300
    shuffle: True
    optimizer: adam
    learning_rate: 1.e-2
    valid_freq: 1
    use_tfb: False
    metrics: [ 'acc', 'rmse' ]
    seed: 2019
    gpu: 0
    lr_scheduler: True
    weight_decay: 0.
    warmup_pct: 0.01
    lr_decay_style: 'cosine'
  model_config:
    hidden_size: 128
    loss_integral_num_sample_per_step: 10
    use_mc_samples: True



SAHP_train:
  base_config:
    stage: train
    backend: torch
    dataset_id: taxi
    runner_id: std_tpp
    model_id: SAHP
    base_dir: './checkpoints/'
  trainer_config:
    batch_size: 256
    max_epoch: 300
    shuffle: True
    optimizer: adam
    learning_rate: 1.e-2
    valid_freq: 1
    use_tfb: False
    metrics: [ 'acc', 'rmse' ]
    seed: 2019
    gpu: 0
    lr_scheduler: True
    weight_decay: 0.
    warmup_pct: 0.01
    lr_decay_style: 'cosine'
  model_config:
    hidden_size: 16
    num_layers: 2
    num_heads: 4
    loss_integral_num_sample_per_step: 10
    use_mc_samples: True
    use_ln: False


THP_train:
  base_config:
    stage: train
    backend: torch
    dataset_id: taxi
    runner_id: std_tpp
    model_id: THP
    base_dir: './checkpoints/'
  trainer_config:
    batch_size: 256
    max_epoch: 300
    shuffle: True
    optimizer: adam
    learning_rate: 1.e-2
    valid_freq: 1
    use_tfb: False
    metrics: [ 'acc', 'rmse' ]
    seed: 2019
    gpu: 0
    lr_scheduler: True
    weight_decay: 0.
    warmup_pct: 0.01
    lr_decay_style: 'cosine'
  model_config:
    hidden_size: 128
    num_layers: 1
    num_heads: 4
    loss_integral_num_sample_per_step: 10
    use_mc_samples: True


AttNHP_train:
  base_config:
    stage: train
    backend: torch
    dataset_id: taxi
    runner_id: std_tpp
    model_id: AttNHP
    base_dir: './checkpoints/'
  trainer_config:
    batch_size: 256
    max_epoch: 300
    shuffle: True
    optimizer: adam
    learning_rate: 1.e-2
    valid_freq: 1
    use_tfb: False
    metrics: [ 'acc', 'rmse' ]
    seed: 2019
    gpu: 0
    lr_scheduler: True
    weight_decay: 0.
    warmup_pct: 0.01
    lr_decay_style: 'cosine'
  model_config:
    hidden_size: 16
    time_emb_size: 16
    num_layers: 3
    num_heads: 4
    loss_integral_num_sample_per_step: 10
    use_mc_samples: True


IntensityFree_train:
  base_config:
    stage: train
    backend: torch
    dataset_id: taxi
    runner_id: std_tpp
    model_id: IntensityFree
    base_dir: './checkpoints/'
  trainer_config:
    batch_size: 256
    max_epoch: 300
    shuffle: True
    optimizer: adam
    learning_rate: 1.e-2
    valid_freq: 1
    use_tfb: False
    metrics: [ 'acc', 'rmse' ]
    seed: 2019
    gpu: 0
    lr_scheduler: True
    weight_decay: 0.
    warmup_pct: 0.1
    lr_decay_style: 'cosine'
  model_config:
    hidden_size: 32
    num_layers: 1
    sharing_param_layer: False
    loss_integral_num_sample_per_step: 10
    use_mc_samples: True
    dropout: 0.0
    use_ln: False
    model_specs:
      num_mix_components: 3

MHP_train:
  base_config:
    stage: train
    backend: torch
    dataset_id: taxi
    runner_id: std_tpp
    model_id: MHP
    base_dir: './checkpoints/'
  trainer_config:
    batch_size: 256
    max_epoch: 300
    shuffle: True
    optimizer: adam
    learning_rate: 1.e-2
    valid_freq: 1
    use_tfb: False
    metrics: [ 'acc', 'rmse' ]
    seed: 2019
    gpu: 0
    lr_scheduler: True
    weight_decay: 0.
    warmup_pct: 0.01
    lr_decay_style: 'cosine'
  model_config:
    hidden_size: 4
    num_layers: 4
    loss_integral_num_sample_per_step: 10
    use_mc_samples: True



S2P2_train:
  base_config:
    stage: train
    backend: torch
    dataset_id: taxi  # Dataset to use, matches the definition at the top of this file.
    runner_id: std_tpp
    model_id: S2P2
    base_dir: './checkpoints/'  # Shere to store model checkpoints upon completion, only the checkpoint with the largest validation log-likelihood will be kept.
  trainer_config:
    batch_size: 256
    max_epoch: 300
    shuffle: True
    optimizer: adam
    learning_rate: 1.e-2
    valid_freq: 1  # How often in epochs is validation / testing performed.
    use_tfb: False
    metrics: [ 'acc', 'rmse' ]
    seed: 2019
    gpu: -1  # ID of GPU to use. Set to -1 to use CPU instead. `mps` backend could lead to incorrect results, please use CPU or CUDA.
    lr_scheduler: True
    weight_decay: 0.
    warmup_pct: 0.01  # Linear warmup period over the first 1% of the training iterations.
    lr_decay_style: 'cosine' # 'cosine' | 'linear' | 'constant'  # After warmup, how does learning rate decay to 0 (if at all).
  model_config:
    hidden_size: 128  # Number of dimensions for u_t and y_t, labeled as H in the paper.
    loss_integral_num_sample_per_step: 10  # How many time points to use to estimate the integrated intensity between each pair of subsequent events for the log-likelihood.
    use_mc_samples: True  # Use Monte-Carlo sampling for the integral estimation. If False, uses a quadrature with a grid of evenly spaced points.
    num_layers: 4  # Number of LLH layers
    model_specs:
      P: 16  # Number of dimensions for the hidden state x_t, labeled as P in the paper.
      dropout_rate: 0.1  # Dropout rate, used immediately after the activation function between layers but before the normalization. Formally, we set u^{(l+1)}_t = LayerNorm(dropout(\sigma(y^{(l)}_t)) + u^{(l)}_t).
      act_func: gelu  # gelu | half_glu | full_glu  # Activation function to use between layers.
      for_loop: False  # If enabled, uses for-loop for computing the recurrence in the LLH layers. If disabled, uses a parallel scan.
      pre_norm: False  # Should be set to False. If True, uses a LayerNorm on the inputs to a LLH layer.
      post_norm: True  # Should be set to True. If True, uses a LayerNorm on the outputs of a LLH layer (after transforming and adding the residual).
      int_forward_variant: False  # Should be set to False. If True, uses u_{t_i} as the ZOH constant for u_t with t \in (t_i, t_{i+1}].
      int_backward_variant: True  # Should be set to True. If True, uses u_{t_{i+1}-} as the ZOH constant for u_t with t \in (t_i, t_{i+1}].
      relative_time: True  # If True, predicts the scaling factor to be applied to the dynamics between each pair of subsequent events. See Sec. 3.3 of the paper.